{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "IIC_implementation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXRaum4ouZ_2"
      },
      "source": [
        "from google.colab import drive\r\n",
        "\r\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5vETs9oogX0"
      },
      "source": [
        "#----- PACKAGES -----\r\n",
        "import random\r\n",
        "from itertools import cycle\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import cv2 \r\n",
        "import os\r\n",
        "from scipy.optimize import linear_sum_assignment as lsa\r\n",
        "from imgaug import augmenters as iaa\r\n",
        "from math import ceil\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import backend as K\r\n",
        "from tensorflow.keras import layers\r\n",
        "from tensorflow.keras.datasets import cifar10\r\n",
        "from tensorflow.keras.models import Model\r\n",
        "from tensorflow.keras.layers import *\r\n",
        "from tensorflow.keras.callbacks import *\r\n",
        "from tensorflow.keras.applications.resnet import ResNet50\r\n",
        "from tensorflow.keras.optimizers import Adam\r\n",
        "from datetime import datetime\r\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\r\n",
        "from scipy.optimize import linear_sum_assignment\r\n",
        "\r\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_wjXj-fOHdw"
      },
      "source": [
        "#Model parameters\r\n",
        "EPOCHS = 501\r\n",
        "STEPS = 80\r\n",
        "BATCH_SIZE=720\r\n",
        "\r\n",
        "INIT_LR = 1e-4\r\n",
        "MAIN_OUTPUT_UNITS=10\r\n",
        "\r\n",
        "CP_HEAD_ITERATIONS=10\r\n",
        "CP_IIC_MODEL_ITERATIONS=50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0zVMj66MibM"
      },
      "source": [
        "#----- DATASET IMPORT -----\r\n",
        "IMG_SIZE=24\r\n",
        "\r\n",
        "[x_train,y_train],[x_test,y_test]=mnist.load_data()\r\n",
        "x_train = x_train.reshape((60000,28,28,1))\r\n",
        "x_test = x_test.reshape((10000,28,28,1))\r\n",
        "\r\n",
        "x_train=x_train.astype(\"float32\")/255\r\n",
        "x_test=x_test.astype(\"float32\")/255\r\n",
        "\r\n",
        "classes=[0,1,2,3,4,5,6,7,8,9]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFOJ1KrrCi-Z"
      },
      "source": [
        "#TRANSFORMATIONS\r\n",
        "\r\n",
        "rotate=tf.keras.Sequential([layers.experimental.preprocessing.RandomRotation(factor=15, fill_mode='nearest', interpolation='bilinear')])\r\n",
        "\r\n",
        "flip=tf.keras.Sequential([layers.experimental.preprocessing.RandomFlip(mode=\"horizontal\")])\r\n",
        "\r\n",
        "resize = tf.keras.Sequential([layers.experimental.preprocessing.Resizing(height=IMG_SIZE,width=IMG_SIZE)])\r\n",
        "\r\n",
        "def crop_transf(batch,crop_fraction):\r\n",
        "  uncropped_size = IMG_SIZE-int((crop_fraction*IMG_SIZE)//100)\r\n",
        "  crop = tf.keras.Sequential([layers.experimental.preprocessing.CenterCrop(height=uncropped_size,width=uncropped_size),layers.experimental.preprocessing.Resizing(height=IMG_SIZE,width=IMG_SIZE)])\r\n",
        "  batch=crop(batch)\r\n",
        "  return batch\r\n",
        "\r\n",
        "def hsv_transf(image):\r\n",
        "\r\n",
        "  val1=random.uniform(0.5,1.5)\r\n",
        "  val2=random.uniform(0.5,1.5)\r\n",
        "  val3=random.uniform(0.5,1.5)\r\n",
        "  \r\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\r\n",
        "  h,s,v = cv2.split(image)\r\n",
        "  h = h*val1\r\n",
        "  s = s*val2\r\n",
        "  v = v*val3\r\n",
        "  cv2.merge([h,s,v],image)\r\n",
        "  cv2.cvtColor(image, cv2.COLOR_HSV2RGB,dst=image)\r\n",
        "\r\n",
        "  return image\r\n",
        "\r\n",
        "\r\n",
        "sobelFilter = K.variable([[[[3,3]], [[0.,10]],[[-3,3]]],\r\n",
        "                          [[[10,0]], [[0.,0]],[[-10,0]]],\r\n",
        "                          [[[3,-3]], [[0.,-10]],[[-3,-3]]]])\r\n",
        "\r\n",
        "\r\n",
        "laplacianFilter = K.variable([[[[0 ]], [[-1 ]],[[0 ]]],\r\n",
        "                      [[[-1 ]], [[4 ]],[[-1 ]]],\r\n",
        "                      [[[0 ]], [[0 ]],[[0 ]]]])\r\n",
        "\r\n",
        "def expandedLaplacian(inputTensor):\r\n",
        "\r\n",
        "    #this considers data_format = 'channels_last'\r\n",
        "    inputChannels = K.reshape(K.ones_like(inputTensor[0,0,0,:]),(1,1,-1,1))\r\n",
        "    #if you're using 'channels_first', use inputTensor[0,:,0,0] above\r\n",
        "\r\n",
        "    return laplacianFilter * inputChannels\r\n",
        "\r\n",
        "def expandedSobel(inputTensor):\r\n",
        "\r\n",
        "    #this considers data_format = 'channels_last'\r\n",
        "    inputChannels = K.reshape(K.ones_like(inputTensor[0,0,0,:]),(1,1,-1,1))\r\n",
        "    #if you're using 'channels_first', use inputTensor[0,:,0,0] above\r\n",
        "\r\n",
        "    return sobelFilter * inputChannels\r\n",
        "\r\n",
        "def laplacian_func(batch,filt=laplacianFilter):\r\n",
        "\r\n",
        "    #get the sobel filter repeated for each input channel\r\n",
        "    #filt = expandedLaplacian(batch)\r\n",
        "    batch=tf.image.rgb_to_grayscale(batch)\r\n",
        "    #calculate the sobel filters for yTrue and yPred\r\n",
        "    #this generates twice the number of input channels \r\n",
        "    #a X and Y channel for each input channel\r\n",
        "    laplacian = K.depthwise_conv2d(batch,filt)\r\n",
        "    resize = tf.keras.Sequential([layers.experimental.preprocessing.Resizing(height=IMG_SIZE,width=IMG_SIZE)])\r\n",
        "    batch=resize(batch)\r\n",
        "    return batch\r\n",
        "\r\n",
        "    #now you just apply the mse:\r\n",
        "    return laplacian\r\n",
        "def converter(x):\r\n",
        "    #x has shape (batch, width, height, channels)\r\n",
        "    return (0.21 * x[:,:,:,:1]) + (0.72 * x[:,:,:,1:2]) + (0.07 * x[:,:,:,-1:])\r\n",
        "\r\n",
        "def sobel_func(batch ):\r\n",
        "    \r\n",
        "    batch=tf.image.rgb_to_grayscale(batch)\r\n",
        "\r\n",
        "    filt=expandedSobel(batch)\r\n",
        "    #calculate the sobel filters for yTrue and yPred\r\n",
        "    #this generates twice the number of input channels \r\n",
        "    #a X and Y channel for each input channel\r\n",
        "    batch = K.depthwise_conv2d(batch,filt)\r\n",
        "    \r\n",
        "    resize = tf.keras.Sequential([layers.experimental.preprocessing.Resizing(height=IMG_SIZE,width=IMG_SIZE)])\r\n",
        "    batch=resize(batch)\r\n",
        "    batch=batch[:,:,:,0]\r\n",
        "    batch=tf.expand_dims(batch,axis=3)\r\n",
        "    return batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYNkOptRtpHe"
      },
      "source": [
        "def data_generator(batch_size=BATCH_SIZE):\r\n",
        "  while True:\r\n",
        "        \r\n",
        "        z=[]\r\n",
        "        z1=[]\r\n",
        "        #Select 21+1 indxs  \r\n",
        "        random_images_indx=random.sample(range(60000),int(batch_size/3))\r\n",
        "\r\n",
        "        #Triplicate each index in order to match the 3 transformations per image\r\n",
        "        triplicated_image_indx=[ i for i in random_images_indx for r in range(3) ]\r\n",
        "\r\n",
        "\r\n",
        "        #21 cropped + 21 multiplied + 21 flipped images\r\n",
        "        transf_samples =x_train[triplicated_image_indx]\r\n",
        "\r\n",
        "        for i in range(len(transf_samples)):\r\n",
        "          rand_val=random.sample(range(3),1)\r\n",
        "          if rand_val==0:\r\n",
        "               transf_samples[i]= crop_transf(transf_samples[i],25)\r\n",
        "          if rand_val==1:\r\n",
        "               transf_samples[i]= flip(transf_samples[i])\r\n",
        "          if rand_val==2:\r\n",
        "               transf_samples[i]= rotate(transf_samples[i])\r\n",
        "            \r\n",
        "        z=np.array(resize(x_train[triplicated_image_indx]))\r\n",
        "        z1=np.array(resize(transf_samples))\r\n",
        "\r\n",
        "        #shuffler = list(np.random.permutation(batch_size))\r\n",
        "        #z = np.array([z[shuffler[i],:,:,:] for i in shuffler])\r\n",
        "        #z1 =  np.array([z1[shuffler[i],:,:,:] for i in shuffler])\r\n",
        "        \r\n",
        "        yield ([z,z1],np.zeros((batch_size,1)).astype(\"float32\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSM0qSGipQ7b"
      },
      "source": [
        "def main_loss(y_true,y_pred,batch_size=BATCH_SIZE,lamb=1):\r\n",
        "\r\n",
        "    k=MAIN_OUTPUT_UNITS\r\n",
        "    y_pred=tf.squeeze(y_pred)\r\n",
        "\r\n",
        "    #Divide the outputs that have been concatenated \r\n",
        "    phi1=tf.squeeze(y_pred[:,0:MAIN_OUTPUT_UNITS])\r\n",
        "    phi2=tf.squeeze(y_pred[:,MAIN_OUTPUT_UNITS:])\r\n",
        "\r\n",
        "    P= tf.reduce_sum(tf.expand_dims(phi1, 2) * tf.expand_dims(phi2, 1), 0)\r\n",
        "\r\n",
        "    #Symmetrize P matrix\r\n",
        "    P=tf.add(P,tf.transpose(P))/2\r\n",
        "\r\n",
        "    #Add eps value in order to avoid 0 values in P\r\n",
        "    P=tf.clip_by_value(P,clip_value_min=1e-6,clip_value_max=1e9)   \r\n",
        "\r\n",
        "    P/=tf.reduce_sum(P)\r\n",
        "\r\n",
        "    pi = tf.broadcast_to(tf.reshape(tf.reduce_sum(P, axis=0), (k, 1)), (k, k))\r\n",
        "    pj = tf.broadcast_to(tf.reshape(tf.reduce_sum(P, axis=1), (1, k)), (k, k))\r\n",
        "    loss = -tf.reduce_sum(P * (tf.math.log(P) - lamb * tf.math.log(pi) - lamb * tf.math.log(pj)))\r\n",
        "\r\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CY91JNgMS6a"
      },
      "source": [
        "def networkB(input,filters):\r\n",
        "\r\n",
        "    X=input\r\n",
        "\r\n",
        "    F1,F2,F3,F4 = filters\r\n",
        "\r\n",
        "    #BLOCK 1\r\n",
        "    X = Conv2D(filters = F1, kernel_size=(5,5), strides =(1,1),padding = 'same', kernel_initializer = 'random_normal', use_bias=False)(X)\r\n",
        "    X = BatchNormalization(axis = -1)(X)\r\n",
        "    X = Activation('relu')(X)\r\n",
        "    X = MaxPool2D(pool_size=(2,2) , strides=(2,2) , padding=\"same\")(X)\r\n",
        "\r\n",
        "    #BLOCK 2\r\n",
        "    X = Conv2D(filters = F2, kernel_size=(3,3), strides =(1,1),padding = 'same', kernel_initializer = 'random_normal', use_bias=False)(X)\r\n",
        "    X = BatchNormalization(axis = -1)(X)\r\n",
        "    X = Activation('relu')(X)\r\n",
        "    X = MaxPool2D(pool_size=(2,2) , strides=(2,2) , padding=\"same\")(X)\r\n",
        "\r\n",
        "    #BLOCK 3\r\n",
        "    X = Conv2D(filters = F3, kernel_size=(3,3), strides =(1,1),padding = 'same', kernel_initializer = 'random_normal', use_bias=False)(X)\r\n",
        "    X = BatchNormalization(axis = -1)(X)\r\n",
        "    X = Activation('relu')(X)\r\n",
        "    X = MaxPool2D(pool_size=(2,2) , strides=(2,2) , padding=\"same\")(X)\r\n",
        "\r\n",
        "    #BLOCK 4\r\n",
        "    X = Conv2D(filters = F4, kernel_size=(3,3), strides =(1,1),padding = 'same', kernel_initializer = 'random_normal', use_bias=False)(X)\r\n",
        "    X = BatchNormalization(axis = -1)(X)\r\n",
        "    X = Activation('relu')(X)\r\n",
        "\r\n",
        "\r\n",
        "    #BLOCK 5\r\n",
        "    X=Flatten()(X)\r\n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5p4H6YEElg9"
      },
      "source": [
        "#Input head model\r\n",
        "input1=Input([IMG_SIZE,IMG_SIZE,1])\r\n",
        "input2=Input([IMG_SIZE,IMG_SIZE,1])\r\n",
        "#input3=Input([IMG_SIZE,IMG_SIZE,1])\r\n",
        "#input4=Input([IMG_SIZE,IMG_SIZE,1])\r\n",
        "#input5=Input([IMG_SIZE,IMG_SIZE,1])\r\n",
        "\r\n",
        "\r\n",
        "base_output_1=networkB(input1,[64,128,256,512])\r\n",
        "#base_output_2=networkB(input2,[64,128,256,512])\r\n",
        "#base_output_3=networkB(input3,[64,128,256,512])\r\n",
        "#base_output_4=networkB(input4,[64,128,256,512])\r\n",
        "#base_output_5=networkB(input5,[64,128,256,512])\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qzG4uMN7vLH"
      },
      "source": [
        "#Define the 5 heads output\r\n",
        "output_1 =Dense(MAIN_OUTPUT_UNITS,activation=\"softmax\", kernel_initializer='random_normal', bias_initializer='zeros')(base_output_1)\r\n",
        "#output_2 =Dense(MAIN_OUTPUT_UNITS,activation=\"softmax\", kernel_initializer='random_normal', bias_initializer='zeros')(base_output_2)\r\n",
        "#output_3 =Dense(MAIN_OUTPUT_UNITS,activation=\"softmax\", kernel_initializer='random_normal', bias_initializer='zeros')(base_output_3)\r\n",
        "#output_4 =Dense(MAIN_OUTPUT_UNITS,activation=\"softmax\", kernel_initializer='random_normal', bias_initializer='zeros')(base_output_4)\r\n",
        "#output_5 =Dense(MAIN_OUTPUT_UNITS,activation=\"softmax\", kernel_initializer='random_normal', bias_initializer='zeros')(base_output_5)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#Define the 5 models related to the 5 heads\r\n",
        "model_1 = Model(inputs=input1, outputs=output_1 , name=\"head_model_1\")\r\n",
        "#model_2 = Model(inputs=input2, outputs=output_2 , name=\"head_model_2\")\r\n",
        "#model_3 = Model(inputs=input3, outputs=output_3 , name=\"head_model_3\")\r\n",
        "#model_4 = Model(inputs=input4, outputs=output_4 , name=\"head_model_4\")\r\n",
        "#model_5 = Model(inputs=input5, outputs=output_5 , name=\"head_model_5\")\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "input_1=Input([IMG_SIZE,IMG_SIZE,1])\r\n",
        "input_2=Input([IMG_SIZE,IMG_SIZE,1])\r\n",
        "\r\n",
        "#Define the two models output\r\n",
        "model_1_out=Concatenate(name=\"model_1_output\")([model_1(input_1),model_1(input_2)])\r\n",
        "#model_2_out=Concatenate(name=\"model_2_output\")([model_2(input_1),model_2(input_2)])\r\n",
        "#model_3_out=Concatenate(name=\"model_3_output\")([model_3(input_1),model_3(input_2)])\r\n",
        "#model_4_out=Concatenate(name=\"model_4_output\")([model_4(input_1),model_4(input_2)])\r\n",
        "#model_5_out=Concatenate(name=\"model_5_output\")([model_5(input_1),model_5(input_2)])\r\n",
        "\r\n",
        "\r\n",
        "#Define the entire model\r\n",
        "IIC_model=Model(inputs=[input_1,input_2],outputs=[model_1_out])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAMe9fcgHot-"
      },
      "source": [
        "#Loss weights callback function definition\r\n",
        "class  CustomCallback(Callback):\r\n",
        "\r\n",
        "    def on_epoch_begin(self, epoch, logs=None):\r\n",
        "      if epoch%CP_HEAD_ITERATIONS==0:\r\n",
        "\r\n",
        "        checkpoint_m1=\"/content/drive/MyDrive/Colab Notebooks/IIC_implementation/cp_head_\"+str(epoch)+\".h5\"\r\n",
        "        model_1.save_weights(checkpoint_m1)\r\n",
        "\r\n",
        "\r\n",
        "      if epoch%CP_IIC_MODEL_ITERATIONS==0:\r\n",
        "        checkpoint_IIC=\"/content/drive/MyDrive/Colab Notebooks/IIC_implementation/cp_IIC_\"+str(epoch)+\".h5\"\r\n",
        "        IIC_model.save_weights(checkpoint_IIC)       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4XufpoROVz4"
      },
      "source": [
        "losses = [main_loss]\r\n",
        "lossWeights = (1)\r\n",
        "opt = Adam(lr=INIT_LR  )\r\n",
        "\r\n",
        "#%load_ext tensorboard\r\n",
        "# Clear any logs from previous runs\r\n",
        "#!rm -rf ./logs/ \r\n",
        "\r\n",
        "IIC_model.compile(optimizer=opt, loss=losses , loss_weights=lossWeights, run_eagerly=False)\r\n",
        "#IIC_model.load_weights(\"/content/drive/MyDrive/Colab Notebooks/IIC_implementation/checkpoint_MAIN_AVG_130.h5\")\r\n",
        "\r\n",
        "#log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\n",
        "#tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\r\n",
        "\r\n",
        "#IIC_model.fit(data_generator(),steps_per_epoch=STEPS,epochs=EPOCHS,verbose=1,callbacks=[CustomCallback(),tensorboard_callback])\r\n",
        "IIC_model.fit(data_generator(),steps_per_epoch=STEPS,epochs=EPOCHS,verbose=1,callbacks=[CustomCallback()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoQFWglQehHf"
      },
      "source": [
        "def  CustomMetric(model,x_true,y_true, num_classes):\r\n",
        "\r\n",
        "    x_true=resize(x_true)\r\n",
        "  \r\n",
        "    #x_true=tf.image.rgb_to_grayscale(x_true)\r\n",
        "    softmax_predictions=model.predict(x_true)\r\n",
        "    predictions=[np.argmax(x) for x in softmax_predictions]\r\n",
        "  \r\n",
        "    # initialize count matrix\r\n",
        "    cnt_mtx = np.zeros([num_classes, num_classes])\r\n",
        "\r\n",
        "    # fill in matrix\r\n",
        "    for i in range(len(y_true)):\r\n",
        "        cnt_mtx[int(predictions[i]), int(y_true[i])] += 1\r\n",
        "\r\n",
        "    # find optimal permutation\r\n",
        "    row_ind, col_ind = linear_sum_assignment(-cnt_mtx)\r\n",
        "    #print(row_ind)\r\n",
        "    #print(col_ind)\r\n",
        "\r\n",
        "    # compute error\r\n",
        "    error = 1 - cnt_mtx[row_ind, col_ind].sum() / cnt_mtx.sum()\r\n",
        "\r\n",
        "    # print results\r\n",
        "    #print('Classification error = {:.4f}'.format(error))\r\n",
        "\r\n",
        "    return error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNh9z4PlX0CP"
      },
      "source": [
        "# -------------SHOW RESULTS--------------\r\n",
        "cp_list=[]\r\n",
        "eval_range=range(0,EPOCHS,CP_HEAD_ITERATIONS)\r\n",
        "for i in eval_range:\r\n",
        "  try:\r\n",
        "    model_1.load_weights(\"/content/drive/MyDrive/Colab Notebooks/IIC_implementation/cp_head_\"+str(i)+\".h5\")\r\n",
        "    cp_list.append(CustomMetric(model_1,x_test,y_test,CP_HEAD_ITERATIONS))\r\n",
        "  except:\r\n",
        "    continue\r\n",
        "    \r\n",
        "plt.plot(eval_range, cp_list) \r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7k9enIQ0ZsiB"
      },
      "source": [
        "#%tensorboard --logdir logs/fit --port=8006\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlT3WUsgyBL6"
      },
      "source": [
        "def convolutional_block(X, filters):\r\n",
        "    \r\n",
        "    # Retrieve Filters\r\n",
        "    F1 = filters\r\n",
        "    \r\n",
        "    # Save the input value\r\n",
        "    X_shortcut = X\r\n",
        "\r\n",
        "    ##### MAIN PATH #####\r\n",
        "    # First component of main path \r\n",
        "    X = Conv2D(filters = F1, kernel_size=(3,3), strides = (2,2) , padding = 'same', kernel_initializer = 'random_normal')(X)\r\n",
        "    X = BatchNormalization(axis = -1)(X)\r\n",
        "    X = Activation('relu')(X)\r\n",
        "\r\n",
        "    # Second component of main path (≈2 lines)\r\n",
        "    X = Conv2D(filters = F1, kernel_size = (3,3), strides = (2,2) , padding = 'same', kernel_initializer = 'random_normal')(X)\r\n",
        "    X = BatchNormalization(axis = -1 )(X)\r\n",
        "\r\n",
        "\r\n",
        "    ##### SHORTCUT PATH #### (≈2 lines)\r\n",
        "    X_shortcut = Conv2D(filters = F1, kernel_size = (3,3), strides = (4,4), padding = 'same', kernel_initializer = 'random_normal')(X_shortcut)\r\n",
        "\r\n",
        "    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\r\n",
        "    X = Add()([X, X_shortcut])\r\n",
        "    X = Activation('relu')(X)\r\n",
        "    \r\n",
        "    return X\r\n",
        "\r\n",
        "def identity_block(X, filters):\r\n",
        "    \r\n",
        "    # Retrieve Filters\r\n",
        "    F1, F2 , F3 = filters\r\n",
        "    \r\n",
        "    # Save the input value\r\n",
        "    X_shortcut = X\r\n",
        "\r\n",
        "    ##### MAIN PATH #####\r\n",
        "    # First component of main path \r\n",
        "    X = Conv2D(filters = F1, kernel_size=(3, 3), strides =(1,1),padding = 'same', kernel_initializer = 'random_normal')(X)\r\n",
        "    X = BatchNormalization(axis = -1)(X)\r\n",
        "    X = Activation('relu')(X)\r\n",
        "\r\n",
        "    # Second component of main path (≈2 lines)\r\n",
        "    X = Conv2D(filters = F2, kernel_size = (3,3), strides = (1,1), padding = 'same', kernel_initializer = 'random_normal')(X)\r\n",
        "    X = BatchNormalization(axis = -1 )(X)\r\n",
        "    X = Activation('relu')(X)\r\n",
        "\r\n",
        "    # third component of main path (≈2 lines)\r\n",
        "    X = Conv2D(filters = F3, kernel_size = (3,3), strides = (1,1), padding = 'same', kernel_initializer = 'random_normal')(X)\r\n",
        "    X = BatchNormalization(axis = -1 )(X)\r\n",
        "    X = Activation('relu')(X)\r\n",
        "\r\n",
        "    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\r\n",
        "    X = Add()([X, X_shortcut])\r\n",
        "    X = Activation('relu')(X)\r\n",
        "    \r\n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}